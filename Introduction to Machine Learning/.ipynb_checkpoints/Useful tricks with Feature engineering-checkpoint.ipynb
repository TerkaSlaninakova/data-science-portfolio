{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering\n",
    "\n",
    "Feature engineering is all about **how to represent the data best** for a particular application.\n",
    "\n",
    "For linear models adding squared or cubed features can help linear models for regression (see Intro to ML chapters 4.2-4.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "adult = pd.read_csv(\"data\\\\adult-dataset\\\\adult.csv\")\n",
    "adult.info()\n",
    "adult = adult[['age', 'workclass', 'education', 'gender', 'hours-per-week', 'occupation', 'income']]\n",
    "display(adult.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling categorical features\n",
    "\n",
    "### One-hot encoding\n",
    "- create new boolean features for every categorical value\n",
    "    - check contents of a column by `value_counts()`\n",
    "    - encode by `get_dummies()`\n",
    "    - important that values in training and testing sets are encoded in the same way\n",
    "    \n",
    "#### Integers as categoricals\n",
    "`get_dummies()` will treat only string values as categorical, where sometimes we want to consider integer values to be categorical as well. In that case we need to typecast it: `demo_df['Integer Feature'] = demo_df['Integer Feature'].astype(str)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adult.workclass.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original features: {list(adult.columns)}\")\n",
    "adult_dummies = pd.get_dummies(adult)\n",
    "print(f\"After encoding features: {list(adult_dummies.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(adult_dummies.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = adult_dummies.drop(['income_<=50K', 'income_>50K'], axis=1).values\n",
    "y = adult_dummies['income_>50K'].values\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "log = LogisticRegression()\n",
    "log.fit(X_train, y_train)\n",
    "print(f\"Test score: {log.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mglearn.datasets.make_wave(n_samples=100)\n",
    "line = np.linspace(-3, 3, 1000, endpoint=False).reshape(-1, 1)\n",
    "plt.plot(X[:, 0], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making linear models more powerful on continuous data - binning, disretization, adding polynomials\n",
    "- skipped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling using `log`, `exp`\n",
    "Linear models and neural networks are tied to scale and distribution of the features. `log` and `exp` can help in relatively scaling the data better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd = np.random.RandomState()\n",
    "rnd.normal(size=(1000,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA\n",
    "Fit PCA and visualize the 2 found principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X_train_scaled)\n",
    "x_pca = pca.transform(X_train_scaled)\n",
    "print(X_train_scaled.shape)\n",
    "print(x_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import mglearn\n",
    "plt.figure(figsize=(8,8))\n",
    "mglearn.discrete_scatter(x_pca[:, 0], x_pca[:, 1], y_train)\n",
    "plt.legend(cancer_dataset.target_names)\n",
    "plt.xlabel(\"First component\")\n",
    "plt.ylabel(\"Second component\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The separation boundary could be fitted quite well even with a simple classifier now. Let's explore the components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.components_.shape)\n",
    "print(pca.components_)\n",
    "\n",
    "plt.matshow(pca.components_)\n",
    "plt.yticks([0,1], [\"First comp.\", \"Second comp.\"])\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(cancer_dataset.feature_names)), cancer_dataset.feature_names, rotation=60)\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.ylabel(\"Principal components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extraction with images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "people_dataset = fetch_lfw_people(min_faces_per_person=20, resize=0.7)\n",
    "fix, axes = plt.subplots(2,5, figsize=(15, 8), subplot_kw={'xticks': (), 'yticks':()})\n",
    "for target, image, ax in zip(people_dataset.target, people_dataset.images, axes.ravel()):\n",
    "    ax.imshow(image)\n",
    "    ax.set_title(people_dataset.target_names[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_dataset.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(people_dataset.data, people_dataset.target, stratify=people_dataset.target)\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train, y_train)\n",
    "print(knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad for 61 classes, but not ideal, let's try PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whitening rescales the principal components to have the same scale.\n",
    "pca = PCA(n_components=100, whiten=True).fit(X_train)\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "print(X_train.shape)\n",
    "print(X_train_pca.shape)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train_pca, y_train)\n",
    "print(knn.score(X_test_pca, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF - Non-negative matrix factorization\n",
    "- looks for non-negative components\n",
    "- particularly helpful if the data is made up from addition of multiple sources (e.g. sound)\n",
    "\n",
    "Similarly to PCA tries to explain the data through a sum of components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = mglearn.datasets.make_signals()\n",
    "plt.figure(figsize=(10,1))\n",
    "plt.plot(S, '-')\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Signal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say i can observe the singal only comping from 3 sources (3 different measurements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "A = np.random.RandomState(0).uniform(size=(100,3))\n",
    "X = np.dot(S, A.T)\n",
    "nmf = NMF(n_components=3)\n",
    "S_ = nmf.fit_transform(X)\n",
    "pca = PCA(n_components=3)\n",
    "H = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [X, S, S_, H]\n",
    "names = ['Observations (first three measurements)',\n",
    "'True sources',\n",
    "'NMF recovered signals',\n",
    "'PCA recovered signals']\n",
    "fig, axes = plt.subplots(4, figsize=(12, 4), gridspec_kw={'hspace': .5}, subplot_kw={'xticks': (), 'yticks': ()})\n",
    "for model, name, ax in zip(models, names, axes):\n",
    "    ax.set_title(name)\n",
    "    ax.plot(model[:, :3], '-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMF did a reasonable job of discovering the true sources, while PCA failed to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE (Manifold learning)\n",
    "- used for visualization, don't generate new features\n",
    "- idea: find 2d representation of the data that preserve the distances betw. points (preserved information indicating whihc points are neighbours to each other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits_dataset = load_digits()\n",
    "fig, axes = plt.subplots(2, 5, figsize=(10, 5), subplot_kw={'xticks':(), 'yticks': ()})\n",
    "for ax, img in zip(axes.ravel(), digits_dataset.images):\n",
    "    ax.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE()\n",
    "digits_tsne = tsne.fit_transform(digits_dataset.data)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.xlim(digits_tsne[:, 0].min(), digits_tsne[:, 0].max() + 1)\n",
    "plt.ylim(digits_tsne[:, 1].min(), digits_tsne[:, 1].max() + 1)\n",
    "for i in range(len(digits_dataset.data)):\n",
    "    plt.text(digits_tsne[i, 0], digits_tsne[i, 1], str(digits_dataset.target[i]))\n",
    "plt.xlabel(\"t-SNE feature 0\")\n",
    "plt.xlabel(\"t-SNE feature 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "### k-means\n",
    "\n",
    "#### Advantages\n",
    "- easy to interpret\n",
    "- fast, scales easily\n",
    "\n",
    "#### Disadvantages\n",
    "- relies on random initialization\n",
    "- restrictive assumtions on the shape of the clusters\n",
    "\n",
    "- assign each data point to the closest center\n",
    "- set each cluster center as meand of the datapoints assigned to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_kmeans_algorithm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "X,y = make_blobs()\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], kmeans.labels_, markers='o')\n",
    "mglearn.discrete_scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], [0, 1, 2],\n",
    "markers='^', markeredgewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-,eans assumes all directions are equally important for each cluster - if the groups are e.g. stretched toward the diagonal, k-means won't perform well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "kmeans.fit(X)\n",
    "\n",
    "y_pred = kmeans.predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap=mglearn.cm2, s=60)\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
    "marker='^', c=[mglearn.cm2(0), mglearn.cm2(1)], s=100, linewidth=2)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-means can be also viewed (in scope of dim. reduction methods mentioned earlierr) as a decomposition method where each group of points is represented using a single component = **vector quantization**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agglomerative clustering\n",
    "- each point is its own cluster, continue merging similar (acc. to linkage criteria) clusters until the specified number of clusters is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mglearn.plots.plot_agglomerative_algorithm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "X, y = make_blobs()\n",
    "agg = AgglomerativeClustering(n_clusters=3)\n",
    "assignment = agg.fit_predict(X)\n",
    "mglearn.discrete_scatter(X[:,0], X[:,1], assignment)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=200)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "agg = AgglomerativeClustering(n_clusters=2)\n",
    "clusters = agg.fit_predict(X_scaled)\n",
    "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To examine the process of joining the clusters, let's plot scipy's dendogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, ward\n",
    "X, y = make_blobs(n_samples=12)\n",
    "# aray of hierarchichal cluster similarities\n",
    "linkage_array = ward(X)\n",
    "dendrogram(linkage_array)\n",
    "ax = plt.gca()\n",
    "bounds = ax.get_xbound()\n",
    "ax.plot(bounds, [7.25, 7.25], '--', c='k')\n",
    "ax.plot(bounds, [4, 4], '--', c='k')\n",
    "ax.text(bounds[1], 7.25, ' two clusters', va='center', fontdict={'size': 15})\n",
    "ax.text(bounds[1], 4, ' three clusters', va='center', fontdict={'size': 15})\n",
    "plt.xlabel(\"Sample index\")\n",
    "plt.ylabel(\"Cluster distance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN\n",
    "\n",
    "#### Advantages\n",
    "- doesn't require set n. of clusters a-priori\n",
    "- can capture complex cluster\n",
    "- can identify outliers\n",
    "\n",
    "#### Disadvantages\n",
    "- somewhat slower\n",
    "\n",
    "#### Parameters\n",
    "- `eps` - what is means for points to be close. very small = no points are 'core', very large = all points forming a single cluster\n",
    "\n",
    "Identifies dense regions in feature space, points within dese regions are called `core` samples. \n",
    "1. pick arbitrary datapoint\n",
    "2. find all points within distance `eps`\n",
    "    - if there are less than `min_samples` points within distance `eps` of the starting point, the point is labeled as noise\n",
    "    - else labeled as `core` and assigned to a new cluster label\n",
    "3. Repeat until there are no more core samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "X, y = make_blobs(n_samples=12)\n",
    "dbscan = DBSCAN()\n",
    "clusters = dbscan.fit_predict(X)\n",
    "clusters\n",
    "# Everything predicted as -1 = noise, because the default parameters are not \n",
    "# suitable for a toy dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it on a `moons` dataset that has proven to be problematic before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=200)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "dbscan = DBSCAN()\n",
    "clusters = dbscan.fit_predict(X_scaled)\n",
    "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "- ARI (adjusted rand index)\n",
    "- NMI (normalized mutual information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "kmeans = KMeans(n_clusters = 2)\n",
    "agglo = AgglomerativeClustering(n_clusters=2)\n",
    "dbscan = DBSCAN()\n",
    "\n",
    "\n",
    "print(\"ARI [kmeans]: {:.2f}\".format(adjusted_rand_score(y, kmeans.fit_predict(X_scaled))))\n",
    "print(\"ARI [agglo]: {:.2f}\".format(adjusted_rand_score(y, agglo.fit_predict(X_scaled))))\n",
    "print(\"ARI [dbscan]: {:.2f}\".format(adjusted_rand_score(y, dbscan.fit_predict(X_scaled))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lfw dataset with clustering algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_people = people_dataset.data\n",
    "y_people = people_dataset.target\n",
    "pca = PCA(n_components=100, whiten=True)\n",
    "pca.fit_transform(X_people)\n",
    "X_pca = pca.transform(X_people)\n",
    "\n",
    "dbscan = DBSCAN(min_samples=3, eps=7)\n",
    "labels_dbscan = dbscan.fit_predict(X_pca)\n",
    "print(labels_dbscan.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in range(max(labels) + 1):\n",
    "    mask = labels_dbscan == cluster\n",
    "    n_images = np.sum(mask)\n",
    "    fig, axes = plt.subplots(1, n_images, figsize=(n_images * 1.5, 4), subplot_kw={'xticks': (), 'yticks': ()})\n",
    "    for image, label, ax in zip(X_people[mask], y_people[mask], axes):\n",
    "        ax.imshow(image.reshape(people_dataset.images[0].shape))\n",
    "        ax.set_title(people_dataset.target_names[label].split()[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=10)\n",
    "labels_km = kmeans.fit_predict(X_pca)\n",
    "print(np.bincount(labels_km))\n",
    "print(labels_km.shape)\n",
    "\n",
    "fig, axes = plt.subplots(2,5, subplot_kw={'xticks': (), 'yticks':()}, figsize=(12, 4))\n",
    "for center, ax in zip(kmeans.cluster_centers_, axes.ravel()):\n",
    "    ax.imshow(pca.inverse_transform(center).reshape(people_dataset.images[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agglomerative = AgglomerativeClustering(n_clusters=40)\n",
    "labels_agg = agglomerative.fit_predict(X_pca)\n",
    "print(\"cluster sizes agglomerative clustering: {}\".format(np.bincount(labels_agg)))\n",
    "n_clusters = 40\n",
    "for cluster in [10, 13, 19, 22, 36]: # hand-picked \"interesting\" clusters\n",
    "    mask = labels_agg == cluster\n",
    "    fig, axes = plt.subplots(1, 15, subplot_kw={'xticks': (), 'yticks': ()},\n",
    "    figsize=(15, 8))\n",
    "    cluster_size = np.sum(mask)\n",
    "    axes[0].set_ylabel(\"#{}: {}\".format(cluster, cluster_size))\n",
    "    for image, label, asdf, ax in zip(X_people[mask], y_people[mask],\n",
    "        labels_agg[mask], axes):\n",
    "        ax.imshow(image.reshape(people_dataset.images[0].shape))\n",
    "        ax.set_title(people_dataset.target_names[label].split()[-1],\n",
    "        fontdict={'fontsize': 9})\n",
    "    for i in range(cluster_size, 15):\n",
    "        axes[i].set_visible(False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
