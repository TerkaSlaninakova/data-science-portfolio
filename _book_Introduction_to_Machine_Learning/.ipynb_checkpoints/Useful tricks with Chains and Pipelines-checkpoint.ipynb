{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains and pipelines\n",
    "Most machine learning applications require not only the application of a single algorithm, but the chaining together of many different processing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1, 'gamma': 1}\n",
      "0.9812206572769953\n",
      "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)\n",
    "\n",
    "scaler = MinMaxScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "\n",
    "params = {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "grid_search = GridSearchCV(SVC(), params, cv=5, return_train_score=True)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem:** the scaling was applied before splitting the dataset inside of GridSearchCV - cross-validation no longer correctly mirror how new data will\n",
    "look to the modeling process\n",
    "- **solution** - `Pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'svm__C': 1, 'svm__gamma': 1}\n",
      "0.9812206572769953\n",
      "Pipeline(memory=None,\n",
      "     steps=[('scaler', MinMaxScaler(copy=True, feature_range=(0, 1))), ('svm', SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma=1, kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))])\n",
      "Test score: 0.97\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipe = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC())])\n",
    "\n",
    "params = {'svm__C': [0.001, 0.01, 0.1, 1, 10, 100], 'svm__gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "grid = GridSearchCV(pipe, param_grid=params, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)\n",
    "print(grid.best_estimator_)\n",
    "print(\"Test score: {:.2f}\".format(grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Leakage - more specific example\n",
    "- regression dataset, 100 samples, 1000 features, sampled from Gaussian distr, y is independent of X, so it shouldn't be possible to learn anything\n",
    "\n",
    "1. select the most informative features using `SelectPercentile`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rnd = np.random.RandomState(seed=0)\n",
    "X = rnd.normal(size=(100, 10000))\n",
    "y = rnd.normal(size=(100,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_selected.shape: (100, 500)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectPercentile, f_regression\n",
    "select = SelectPercentile(score_func=f_regression, percentile=5).fit(X, y)\n",
    "X_selected = select.transform(X)\n",
    "print(\"X_selected.shape: {}\".format(X_selected.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation accuracy (cv only on ridge): 0.91\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import Ridge\n",
    "print(\"Cross-validation accuracy (cv only on ridge): {:.2f}\".format(\n",
    "np.mean(cross_val_score(Ridge(), X_selected, y, cv=5))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is clearly wrong since the data is random without any relationship. Feature selection here picked the features that are well correlated with the target and spoiled the validation set in the CV loop as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation accuracy (pipeline): -0.25\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([(\"select\", SelectPercentile(score_func=f_regression,\n",
    "percentile=5)),\n",
    "(\"ridge\", Ridge())])\n",
    "print(\"Cross-validation accuracy (pipeline): {:.2f}\".format(\n",
    "np.mean(cross_val_score(pipe, X, y, cv=5))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deeper into pipelines\n",
    "- accessing parameters\n",
    "- choosing a model using Grid-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline steps:\n",
      "[('standardscaler-1', StandardScaler(copy=True, with_mean=True, with_std=True)), ('pca', PCA(copy=True, iterated_power='auto', n_components=2, random_state=None,\n",
      "  svd_solver='auto', tol=0.0, whiten=False)), ('standardscaler-2', StandardScaler(copy=True, with_mean=True, with_std=True))]\n",
      "PCA shapes: [[ 0.21890244  0.10372458  0.22753729  0.22099499  0.14258969  0.23928535\n",
      "   0.25840048  0.26085376  0.13816696  0.06436335  0.20597878  0.01742803\n",
      "   0.21132592  0.20286964  0.01453145  0.17039345  0.15358979  0.1834174\n",
      "   0.04249842  0.10256832  0.22799663  0.10446933  0.23663968  0.22487053\n",
      "   0.12795256  0.21009588  0.22876753  0.25088597  0.12290456  0.13178394]\n",
      " [-0.23385713 -0.05970609 -0.21518136 -0.23107671  0.18611302  0.15189161\n",
      "   0.06016536 -0.0347675   0.19034877  0.36657547 -0.10555215  0.08997968\n",
      "  -0.08945723 -0.15229263  0.20443045  0.2327159   0.19720728  0.13032156\n",
      "   0.183848    0.28009203 -0.21986638 -0.0454673  -0.19987843 -0.21935186\n",
      "   0.17230435  0.14359317  0.09796411 -0.00825724  0.14188335  0.27533947]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "pipe = make_pipeline(StandardScaler(), PCA(n_components=2), StandardScaler())\n",
    "print(\"Pipeline steps:\\n{}\".format(pipe.steps))\n",
    "pipe.fit(cancer.data)\n",
    "print(f\"PCA shapes: {pipe.named_steps.pca.components_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores:  [0.96666667 1.         0.96666667 0.96666667 1.        ]\n",
      "Mean cross-validation score:  0.9800000000000001\n",
      "Cross-validation scores [roc_auc]:  [1. 1. 1. 1. 1.]\n",
      "Mean cross-validation score [roc_auc]:  1.0\n"
     ]
    }
   ],
   "source": [
    "params = {'C': [0.001, 0.01, 0.1, 1, 10, 100], 'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(GridSearchCV(SVC(), params, cv=5),\n",
    "iris.data, iris.target, cv=5)\n",
    "print(\"Cross-validation scores: \", scores)\n",
    "print(\"Mean cross-validation score: \", scores.mean())\n",
    "\n",
    "scores = cross_val_score(GridSearchCV(SVC(), params, cv=5),\n",
    "iris.data, iris.target == 0, cv=5, scoring=\"roc_auc\")\n",
    "print(\"Cross-validation scores [roc_auc]: \", scores)\n",
    "print(\"Mean cross-validation score [roc_auc]: \", scores.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
